{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T18:30:47.463734Z",
     "iopub.status.busy": "2025-05-03T18:30:47.463341Z",
     "iopub.status.idle": "2025-05-03T18:30:49.047056Z",
     "shell.execute_reply": "2025-05-03T18:30:49.046082Z",
     "shell.execute_reply.started": "2025-05-03T18:30:47.463695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create base user-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T18:30:49.048599Z",
     "iopub.status.busy": "2025-05-03T18:30:49.047964Z",
     "iopub.status.idle": "2025-05-03T18:30:49.053309Z",
     "shell.execute_reply": "2025-05-03T18:30:49.052097Z",
     "shell.execute_reply.started": "2025-05-03T18:30:49.048561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created functions that help me to clean data and have it in good shape\n",
    "* clean_text - Cleans text of non-letter characters (e.g., spaces, punctuation marks) at the beginning and end of a line.\n",
    "* add_space - Adds a period with a space between a lowercase letter and an uppercase letter if they follow each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T18:31:22.355135Z",
     "iopub.status.busy": "2025-05-03T18:31:22.354786Z",
     "iopub.status.idle": "2025-05-03T18:31:22.359796Z",
     "shell.execute_reply": "2025-05-03T18:31:22.358519Z",
     "shell.execute_reply.started": "2025-05-03T18:31:22.355111Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return re.sub(r'^[^\\wА-Яа-яІіЇїЄє]+|[^\\wА-Яа-яІіЇїЄє]+$', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T18:31:22.730355Z",
     "iopub.status.busy": "2025-05-03T18:31:22.730029Z",
     "iopub.status.idle": "2025-05-03T18:31:22.734980Z",
     "shell.execute_reply": "2025-05-03T18:31:22.734040Z",
     "shell.execute_reply.started": "2025-05-03T18:31:22.730333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_space(text):\n",
    "    return re.sub(r'([а-я])([А-Я])', r'\\1. \\2', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How scrape_website work\n",
    "* We get each field by html tags or classes\n",
    "* By link we can get to item page and get additional info like:\n",
    "    - Shops where paople can buy this item\n",
    "    - Item characteristics\n",
    "    - Item description\n",
    "    - Item image link\n",
    "    - Item path that can give useful info like category or subcategory\n",
    "* Characteristics and shops are in json format for comfortable use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T18:31:24.821236Z",
     "iopub.status.busy": "2025-05-03T18:31:24.820885Z",
     "iopub.status.idle": "2025-05-03T18:31:24.833143Z",
     "shell.execute_reply": "2025-05-03T18:31:24.832068Z",
     "shell.execute_reply.started": "2025-05-03T18:31:24.821208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pattern = r\"Сподобалось:\\s*(.*?)\\s*Не сподобалося:\\s*(.*?)\\s*Досвід використання:\\s*(.*)\"\n",
    "def scrape_website(review):\n",
    "    name = review.find(class_=\"reviews-info-product__title\").text.strip() or np.nan\n",
    "\n",
    "    ID = hashlib.md5(name.encode()).hexdigest()\n",
    "\n",
    "    link = \"https://hotline.ua\" + review.find(class_=\"reviews-info-product__title\")[\"href\"]\n",
    "\n",
    "    response_link = requests.get(link, headers=headers).text\n",
    "    response_html = BeautifulSoup(response_link, \"html.parser\")\n",
    "    img_tag = response_html.find(\"img\", attrs={\"data-tracking-id\": \"product-3\"})\n",
    "    pic = (\"https://hotline.ua\" + img_tag[\"src\"]) or np.nan\n",
    "\n",
    "    recommend = review.find(\"div\", class_=[\"review__recommend review__recommend--like\", \n",
    "                                           \"review__recommend review__recommend--dislike\"])\n",
    "    if recommend:\n",
    "        recommend = recommend.text.strip()\n",
    "    else:\n",
    "        recommend = np.nan\n",
    "\n",
    "    description = review.find(\"div\", class_=\"review__row-experience\").text.strip().replace(\"\\n\", \".\") or np.nan\n",
    "    match = re.search(pattern, description, re.DOTALL)\n",
    "    if match:\n",
    "        liked = clean_text(match.group(1).strip())\n",
    "        disliked = clean_text(match.group(2).strip())\n",
    "        experience = clean_text(match.group(3).strip())\n",
    "    else:\n",
    "        print(\"Failed to parse the text.\")\n",
    "\n",
    "    comment = review.find(\"div\", class_=\"review__row-comment\").text.strip() or np.nan\n",
    "\n",
    "    shops = response_html.find_all(\"div\", class_=\"list__item flex content\")\n",
    "    shops_list = []\n",
    "    for shop in shops:\n",
    "        title = shop.find(\"a\", class_=\"shop__title\")\n",
    "        shop_name = title.text.strip()\n",
    "        shop_link = \"https://hotline.ua\" + title[\"href\"]\n",
    "        shop_img = \"https://hotline.ua\" + shop.find(class_=\"shop__img-container\").find(\"img\")[\"src\"]\n",
    "\n",
    "        shops_list.append({\n",
    "                \"name\": shop_name,\n",
    "                \"link\": shop_link,\n",
    "                \"image\": shop_img\n",
    "            })\n",
    "    shops_json = json.dumps(shops_list, ensure_ascii=False, indent=4)\n",
    "\n",
    "    path = response_html.find(\"ul\", class_=\"breadcrumbs__list\").text.strip().replace(\"\\n          \", \"/\").replace(\"//\",\"/\")\n",
    "\n",
    "    characteristics_table = response_html.find(class_=\"specifications__table\")\n",
    "    characteristics = {}\n",
    "    for tr in characteristics_table.find_all(\"tr\"):\n",
    "        try:\n",
    "            data = tr.find_all(\"td\")\n",
    "            characteristics[data[0].contents[0].strip().strip()] = data[1].text.strip()\n",
    "        except:\n",
    "            pass\n",
    "    characteristics_json = json.dumps(characteristics, ensure_ascii=False, indent=4)\n",
    "\n",
    "    description = response_html.find(class_=\"html-clamp description__content\").text.strip()\n",
    "    return name, ID, link, pic, recommend, liked, disliked, experience, comment, shops_json, path, characteristics_json, description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame we want to get data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T18:31:28.217032Z",
     "iopub.status.busy": "2025-05-03T18:31:28.216713Z",
     "iopub.status.idle": "2025-05-03T18:31:28.227954Z",
     "shell.execute_reply": "2025-05-03T18:31:28.226942Z",
     "shell.execute_reply.started": "2025-05-03T18:31:28.217009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "reviews_df = pd.DataFrame(columns=[\"name\", \"ID\", \"link\", \"pic\", \"recommend\", \"liked\", \"disliked\",\n",
    "                                   \"experience\", \"comment\", \"shops_json\", \"path\", \"characteristics_json\", \"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T18:31:28.475596Z",
     "iopub.status.busy": "2025-05-03T18:31:28.475166Z",
     "iopub.status.idle": "2025-05-03T18:31:34.206350Z",
     "shell.execute_reply": "2025-05-03T18:31:34.205130Z",
     "shell.execute_reply.started": "2025-05-03T18:31:28.475563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install fake_useragent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red; font-size: 36px\"><b>IF YOU NEED ONLY REVIEWS READ THE TEXT BELOW AND GO FURTHER</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How i process each page (All data for full dataset)\n",
    "* We need to create random user-agent to reduce chance of being detected as a bot. It is a better practice but i tested it on pre-defined user-agent and it worked the same.\n",
    "* Our code prints number of page which is processing and if something wrong it prints out the message of error . In our case every message means that format is not appropriate for web scraping and this review will be skiped. So we get the reviews we need\n",
    "* Hotline gives only 13-14 pages to process and after that it gives message of error that means we need to stop running cell and save our DataFrame to .csv format. This doesn't stop running cell but it's a sign that we can get more data now. Later (1-2 hours you can get data again). I think that 13-14 pages of reviews is enough, because its 110-130 reviews.\n",
    "* To run cell below you need to choose page you want to start from:\n",
    "  - for page in range( \"YOUR_START_PAGE\", 100)\n",
    "  - \"YOUR_LINK\" should be in this format from Hotline: https://hotline.ua/ua/reviews/products/. You can change categories that are on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "ua = UserAgent()\n",
    "for page in range(\"YOUR_START_PAGE\", \"YOUR_END_PAGE\"):\n",
    "    headers = {\"User-Agent\": ua.random}\n",
    "    print(page)\n",
    "    webpage_url = 'YOUR_LINK' + f'&p={page}'\n",
    "    response = requests.get(webpage_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    reviews = soup.find_all(\"div\", class_=\"reviews-info__item content\")\n",
    "    for review in reviews:\n",
    "        try:\n",
    "            name, ID, link, pic, recommend, liked, disliked, experience, comment, shops_json, path, characteristics_json, description = scrape_website(review)\n",
    "            review_data = pd.DataFrame([{\"name\": name,\n",
    "                                         \"ID\": ID,\n",
    "                                        \"link\": link,\n",
    "                                        \"pic\": pic,\n",
    "                                        \"recommend\": recommend,\n",
    "                                        \"liked\": liked,\n",
    "                                        \"disliked\": disliked,\n",
    "                                        \"experience\": experience,\n",
    "                                        \"comment\": comment,\n",
    "                                        \"shops_json\": shops_json,\n",
    "                                        \"path\": path,\n",
    "                                        \"characteristics_json\": characteristics_json,\n",
    "                                        \"description\": description}])\n",
    "            reviews_df = pd.concat([reviews_df, review_data], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    sleep_time = random.randint(3, 7)\n",
    "    time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistic of web scraping\n",
    "* 1-14 before error message\n",
    "* 15-28 before error message\n",
    "* 29-42 before error message\n",
    "* 43-56 before error message\n",
    "* 57-68 before error message\n",
    "* 69-80 before error message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finaly you need to change name of .csv file and it will be succesfully saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T20:56:41.212506Z",
     "iopub.status.busy": "2025-03-08T20:56:41.212153Z",
     "iopub.status.idle": "2025-03-08T20:56:41.218676Z",
     "shell.execute_reply": "2025-03-08T20:56:41.217548Z",
     "shell.execute_reply.started": "2025-03-08T20:56:41.212478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "reviews_df.to_csv(\"YOUR_CSV.csv\", index=False, encoding=\"utf-8-sig\", quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How i process each page (Only reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything is the same as it was but without info about item because it was the reason why we can't get data after 14 pages collecting our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T18:31:34.208736Z",
     "iopub.status.busy": "2025-05-03T18:31:34.208327Z",
     "iopub.status.idle": "2025-05-03T18:31:34.219938Z",
     "shell.execute_reply": "2025-05-03T18:31:34.218750Z",
     "shell.execute_reply.started": "2025-05-03T18:31:34.208699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pattern = r\"Сподобалось:\\s*(.*?)\\s*Не сподобалося:\\s*(.*?)\\s*Досвід використання:\\s*(.*)\"\n",
    "def parse_only_reviews(review):\n",
    "    name = review.find(class_=\"reviews-info-product__title\").text.strip() or np.nan\n",
    "\n",
    "    ID = hashlib.md5(name.encode()).hexdigest()\n",
    "\n",
    "    link = \"https://hotline.ua\" + review.find(class_=\"reviews-info-product__title\")[\"href\"]\n",
    "    recommendation = review.find(\"div\", class_=[\"review__recommend review__recommend--like\",\n",
    "                                           \"review__recommend review__recommend--dislike\"])\n",
    "    if recommendation:\n",
    "        recommend = recommendation.text.strip()\n",
    "    else:\n",
    "        print(\"Recommend not found\")\n",
    "\n",
    "    description = review.find(\"div\", class_=\"review__row-experience\").text.strip().replace(\"\\n\", \".\") or np.nan\n",
    "    match = re.search(pattern, description, re.DOTALL)\n",
    "    if match:\n",
    "        liked = clean_text(match.group(1).strip())\n",
    "        disliked = clean_text(match.group(2).strip())\n",
    "        experience = clean_text(match.group(3).strip())\n",
    "    else:\n",
    "        print(\"Не вдалося розпарсити текст.\")\n",
    "\n",
    "    comment = review.find(\"div\", class_=\"review__row-comment\").text.strip() or np.nan\n",
    "    return name, ID, link, recommend, liked, disliked, experience, comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T18:31:35.164434Z",
     "iopub.status.busy": "2025-05-03T18:31:35.163927Z",
     "iopub.status.idle": "2025-05-03T18:31:35.173201Z",
     "shell.execute_reply": "2025-05-03T18:31:35.171852Z",
     "shell.execute_reply.started": "2025-05-03T18:31:35.164367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "only_reviews = pd.DataFrame(columns=[\"name\", \"ID\", \"link\", \"recommend\", \"liked\", \"disliked\", \"experience\", \"comment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run cell below you need to choose page you want to start from:\n",
    "  - for page in range( \"YOUR_START_PAGE\", \"YOUR_END_PAGE\")\n",
    "  - \"YOUR_LINK\" should be in this format from Hotline: https://hotline.ua/ua/reviews/products/. You can change categories that are on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T21:01:50.493207Z",
     "iopub.status.busy": "2025-05-03T21:01:50.490738Z",
     "iopub.status.idle": "2025-05-03T21:08:44.408482Z",
     "shell.execute_reply": "2025-05-03T21:08:44.407582Z",
     "shell.execute_reply.started": "2025-05-03T21:01:50.493149Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "ua = UserAgent()\n",
    "def parse_reviews_url_list(url_list):\n",
    "    for url in url_list:\n",
    "        only_reviews = pd.DataFrame(columns=[\"name\", \"ID\", \"link\", \"recommend\", \"liked\", \"disliked\", \"experience\", \"comment\"])\n",
    "        headers_pages = {\"User-Agent\": ua.random}\n",
    "        response_pages = requests.get(url, headers=headers_pages)\n",
    "        soup_pages = BeautifulSoup(response_pages.text, \"html.parser\")\n",
    "        pages = soup_pages.find_all(class_=\"page\")\n",
    "        pages = [page.text.strip() for page in pages]\n",
    "        pages = list(filter(lambda x: x not in [\"\", \"...\"], pages))\n",
    "        pages = [int(x) for x in pages]\n",
    "        for page in range(min(pages), max(pages)+1):\n",
    "            headers = {\"User-Agent\": ua.random}\n",
    "            print(page)\n",
    "            webpage_url = url + f'&p={page}'\n",
    "            response = requests.get(webpage_url, headers=headers)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            reviews = soup.find_all(\"div\", class_=\"reviews-info__item content\")\n",
    "            for review in reviews:\n",
    "                try:\n",
    "                    name, ID, link, recommend, liked, disliked, experience, comment = parse_only_reviews(review)\n",
    "                    review_data = pd.DataFrame([{\"name\": name,\n",
    "                                                \"ID\": ID,\n",
    "                                                \"link\": link,\n",
    "                                                \"recommend\": recommend,\n",
    "                                                \"liked\": liked,\n",
    "                                                \"disliked\": disliked,\n",
    "                                                \"experience\": experience,\n",
    "                                                \"comment\": comment}])\n",
    "                    only_reviews = pd.concat([only_reviews, review_data], ignore_index=True)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    pass\n",
    "            sleep_time = random.randint(5, 21)\n",
    "            time.sleep(sleep_time)\n",
    "        query = urlparse(url).query\n",
    "        params = parse_qs(query)\n",
    "        vendor_id = params.get(\"vendor_id\", [\"tablets\"])[0] #Change default if u need\n",
    "        only_reviews.to_csv(f\"new_parse/vendor_id_{vendor_id}.csv\", index=False, encoding=\"utf-8-sig\", quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: red; font-size: 25px\"><b>The link should have at least 2 pages of reviews to work properly!!!</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = [] # Your url list\n",
    "parse_reviews_url_list(url_list)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
