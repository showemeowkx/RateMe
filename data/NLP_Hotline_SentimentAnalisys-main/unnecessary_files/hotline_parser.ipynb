{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-08T12:34:38.720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create base user-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T13:23:22.295577Z",
     "iopub.status.busy": "2025-03-08T13:23:22.295133Z",
     "iopub.status.idle": "2025-03-08T13:23:22.300778Z",
     "shell.execute_reply": "2025-03-08T13:23:22.299528Z",
     "shell.execute_reply.started": "2025-03-08T13:23:22.295535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created functions that help me to clean data and have it in good shape\n",
    "* clean_text - Cleans text of non-letter characters (e.g., spaces, punctuation marks) at the beginning and end of a line.\n",
    "* add_space - Adds a period with a space between a lowercase letter and an uppercase letter if they follow each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-08T12:34:38.720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return re.sub(r'^[^\\wА-Яа-яІіЇїЄє]+|[^\\wА-Яа-яІіЇїЄє]+$', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-08T12:34:38.720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_space(text):\n",
    "    return re.sub(r'([а-я])([А-Я])', r'\\1. \\2', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How scrape_website work\n",
    "* We get each field by html tags or classes\n",
    "* By link we can get to item page and get additional info like:\n",
    "    - Shops where paople can buy this item\n",
    "    - Item characteristics\n",
    "    - Item description\n",
    "    - Item image link\n",
    "    - Item path that can give useful info like category or subcategory\n",
    "* Characteristics and shops are in json format for comfortable use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-08T12:34:38.720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pattern = r\"Сподобалось:\\s*(.*?)\\s*Не сподобалося:\\s*(.*?)\\s*Досвід використання:\\s*(.*)\"\n",
    "def scrape_website(review):\n",
    "    name = review.find(class_=\"reviews-info-product__title\").text.strip() or np.nan\n",
    "\n",
    "    ID = hashlib.md5(name.encode()).hexdigest()\n",
    "    \n",
    "    link = \"https://hotline.ua\" + review.find(class_=\"reviews-info-product__title\")[\"href\"]\n",
    "    \n",
    "    response_link = requests.get(link, headers=headers).text\n",
    "    response_html = BeautifulSoup(response_link, \"html.parser\")\n",
    "    img_tag = response_html.find(\"img\", attrs={\"data-tracking-id\": \"product-3\"})\n",
    "    pic = (\"https://hotline.ua\" + img_tag[\"src\"]) or np.nan\n",
    "    \n",
    "    recommend = review.find(\"div\", class_=[\"review__recommend review__recommend--like\", \n",
    "                                           \"review__recommend review__recommend--dislike\"])\n",
    "    if recommend:\n",
    "        recommend = recommend.text.strip()\n",
    "    else:\n",
    "        recommend = np.nan\n",
    "    \n",
    "    description = review.find(\"div\", class_=\"review__row-experience\").text.strip().replace(\"\\n\", \".\") or np.nan\n",
    "    match = re.search(pattern, description, re.DOTALL)\n",
    "    if match:\n",
    "        liked = clean_text(match.group(1).strip())\n",
    "        disliked = clean_text(match.group(2).strip())\n",
    "        experience = clean_text(match.group(3).strip())\n",
    "    else:\n",
    "        print(\"Не вдалося розпарсити текст.\")\n",
    "        \n",
    "    comment = review.find(\"div\", class_=\"review__row-comment\").text.strip() or np.nan\n",
    "\n",
    "    shops = response_html.find_all(\"div\", class_=\"list__item flex content\")\n",
    "    shops_list = []\n",
    "    for shop in shops:\n",
    "        title = shop.find(\"a\", class_=\"shop__title\")\n",
    "        shop_name = title.text.strip()\n",
    "        shop_link = \"https://hotline.ua\" + title[\"href\"]\n",
    "        shop_img = \"https://hotline.ua\" + shop.find(class_=\"shop__img-container\").find(\"img\")[\"src\"]\n",
    "        \n",
    "        shops_list.append({\n",
    "                \"name\": shop_name,\n",
    "                \"link\": shop_link,\n",
    "                \"image\": shop_img\n",
    "            })\n",
    "    shops_json = json.dumps(shops_list, ensure_ascii=False, indent=4)\n",
    "\n",
    "    path = response_html.find(\"ul\", class_=\"breadcrumbs__list\").text.strip().replace(\"\\n          \", \"/\").replace(\"//\",\"/\")\n",
    "\n",
    "    characteristics_table = response_html.find(class_=\"specifications__table\")\n",
    "    characteristics = {}\n",
    "    for tr in characteristics_table.find_all(\"tr\"):\n",
    "        try:\n",
    "            data = tr.find_all(\"td\")\n",
    "            characteristics[data[0].contents[0].strip().strip()] = data[1].text.strip()\n",
    "        except:\n",
    "            pass\n",
    "    characteristics_json = json.dumps(characteristics, ensure_ascii=False, indent=4)\n",
    "\n",
    "    description = response_html.find(class_=\"html-clamp description__content\").text.strip()\n",
    "    return name, ID, link, pic, recommend, liked, disliked, experience, comment, shops_json, path, characteristics_json, description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame we want to get data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-08T12:34:38.720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "reviews_df = pd.DataFrame(columns=[\"name\", \"ID\", \"link\", \"pic\", \"recommend\", \"liked\", \"disliked\",\n",
    "                                   \"experience\", \"comment\", \"shops_json\", \"path\", \"characteristics_json\", \"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-08T12:34:38.720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install fake_useragent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How i process each page\n",
    "* We need to create random user-agent to reduce chance of being detected as a bot. It is a better practice but i tested it on pre-defined user-agent and it worked the same.\n",
    "* Our code prints number of page which is processing and if something wrong it prints out the message of error . In our case every message means that format is not appropriate for web scraping and this review will be skiped. So we get the reviews we need\n",
    "* Hotline gives only 13-14 pages to process and after that it gives message of error that means we need to stop running cell and save our DataFrame to .csv format. This doesn't stop running cell but it's a sign that we can get more data now. Later (1-2 hours you can get data again). I think that 13-14 pages of reviews is enough, because its 110-130 reviews.\n",
    "* To run cell below you need to choose page you want to start from:\n",
    "  - for page in range( \"YOUR_START_PAGE\", 100):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-08T12:34:38.720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "ua = UserAgent()\n",
    "for page in range(1, 100):\n",
    "    headers = {\"User-Agent\": ua.random}\n",
    "    print(page)\n",
    "    webpage_url = f'https://hotline.ua/ua/reviews/products/?section_id=11&vendor_id=242&p={page}'\n",
    "    response = requests.get(webpage_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    reviews = soup.find_all(\"div\", class_=\"reviews-info__item content\")\n",
    "    for review in reviews:\n",
    "        try:\n",
    "            name, ID, link, pic, recommend, liked, disliked, experience, comment, shops_json, path, characteristics_json, description = scrape_website(review)\n",
    "            review_data = pd.DataFrame([{\"name\": name,\n",
    "                                         \"ID\": ID,\n",
    "                                        \"link\": link,\n",
    "                                        \"pic\": pic,\n",
    "                                        \"recommend\": recommend,\n",
    "                                        \"liked\": liked,\n",
    "                                        \"disliked\": disliked,\n",
    "                                        \"experience\": experience,\n",
    "                                        \"comment\": comment,\n",
    "                                        \"shops_json\": shops_json,\n",
    "                                        \"path\": path,\n",
    "                                        \"characteristics_json\": characteristics_json,\n",
    "                                        \"description\": description}])\n",
    "            reviews_df = pd.concat([reviews_df, review_data], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    sleep_time = random.randint(3, 7)\n",
    "    time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistic of web scraping\n",
    "* 1-14 before error message\n",
    "* 15-28 before error message\n",
    "* 29-42 before error message\n",
    "* 43-56 before error message\n",
    "* 57-68 before error message\n",
    "* 69-80 before error message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finaly you need to change name of .csv file and it will be succesfully saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "reviews_df.to_csv(\"YOUR_CSV.csv\", index=False, encoding=\"utf-8-sig\", quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "moto = pd.read_csv(\"motorola_1_65_only_reviews.csv\")\n",
    "xiaomi1 = pd.read_csv(\"xiaomi_1_100_only_reviews.csv\")\n",
    "xiaomi2 = pd.read_csv(\"xiaomi_100_220_only_reviews.csv\")\n",
    "zte = pd.read_csv(\"zte_1_12_only_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "final_dataset = pd.concat([xiaomi1, xiaomi2, moto, zte], ignore_index=True)\n",
    "final_dataset.to_csv(\"reviews_for_nlp.csv\", index=False, encoding=\"utf-8-sig\", quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
